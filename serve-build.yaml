# This file was generated using the `serve build` command on Ray v3.0.0.dev0.

import_path: rayviary.app:entrypoint

host: 0.0.0.0

port: 8000

deployments:

- name: LLMDeployment
  max_concurrent_queries: 2
  autoscaling_config:
    min_replicas: 1
    initial_replicas: 1
    max_replicas: 8
    target_num_ongoing_requests_per_replica: 1.0
    metrics_interval_s: 10.0
    look_back_period_s: 30.0
    smoothing_factor: 1.0
    downscale_delay_s: 300.0
    upscale_delay_s: 30.0
  ray_actor_options:
    resources:
      worker_node: 0.5
  user_config:
    hf_home: /nvme/cache
    model_config:
      max_batch_size: 2
      dtype: float16
      from_pretrained_kwargs: {}
      generation_kwargs:
        do_sample: true
        max_new_tokens: 512
        temperature: 1.0
        top_p: 1.0
        return_token_type_ids: false
      mirror_bucket_uri: s3://large-dl-models-mirror/restricted/models--CarperAI--stable-vicuna-13b-delta/main/
      mode:
        type: DeepSpeed
        use_kernel: false
      name: CarperAI/stable-vicuna-13b-delta
      pipeline_cls: stablelm
      prompt_format: "### Assistant: I am StableVicuna, a large language model created by CarperAI. I am here to chat!\n### Human: {instruction}\n### Assistant: "
      stopping_tokens: [835, [13, 2277]]
    scaling_config:
      num_workers: 2
      num_gpus_per_worker: 1
      num_cpus_per_worker: 4

- name: LLMDeployment_1
  max_concurrent_queries: 2
  autoscaling_config:
    min_replicas: 1
    initial_replicas: 1
    max_replicas: 8
    target_num_ongoing_requests_per_replica: 1.0
    metrics_interval_s: 10.0
    look_back_period_s: 30.0
    smoothing_factor: 1.0
    downscale_delay_s: 300.0
    upscale_delay_s: 30.0
  ray_actor_options:
    resources:
      worker_node: 0.5
  user_config:
    hf_home: /nvme/cache
    model_config:
      max_batch_size: 2
      dtype: float16
      from_pretrained_kwargs: {}
      generation_kwargs:
        do_sample: true
        max_new_tokens: 512
        temperature: 0.7
        return_token_type_ids: false
      mirror_bucket_uri: s3://large-dl-models-mirror/restricted/models--lmsys--vicuna-13b-delta-v1.1/main/
      mode:
        type: DeepSpeed
        use_kernel: false
      name: lmsys/vicuna-13b-delta-v1.1
      pipeline_cls: stablelm
      prompt_format: "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. USER: {instruction} ASSISTANT: "
      stopping_tokens: []
    scaling_config:
      num_workers: 2
      num_gpus_per_worker: 1
      num_cpus_per_worker: 4

- name: LLMDeployment_2
  max_concurrent_queries: 20
  autoscaling_config:
    min_replicas: 1
    initial_replicas: 1
    max_replicas: 8
    target_num_ongoing_requests_per_replica: 1.0
    metrics_interval_s: 10.0
    look_back_period_s: 30.0
    smoothing_factor: 1.0
    downscale_delay_s: 300.0
    upscale_delay_s: 30.0
  ray_actor_options:
    resources:
      worker_node: 0.5
  user_config:
    hf_home: /nvme/cache
    model_config:
      max_batch_size: 20
      dtype: float16
      from_pretrained_kwargs: {}
      generation_kwargs:
        do_sample: true
        max_new_tokens: 256
        temperature: 0.7
      mirror_bucket_uri: s3://large-dl-models-mirror/models--stabilityai--stablelm-tuned-alpha-7b/main/
      mode:
        type: DeepSpeed
        use_kernel: true
      name: stabilityai/stablelm-tuned-alpha-7b
      pipeline_cls: stablelm
      prompt_format: null
      stopping_tokens: null
    scaling_config:
      num_workers: 2
      num_gpus_per_worker: 1
      num_cpus_per_worker: 4

- name: LLMDeployment_3
  max_concurrent_queries: 2
  autoscaling_config:
    min_replicas: 1
    initial_replicas: 1
    max_replicas: 8
    target_num_ongoing_requests_per_replica: 1.0
    metrics_interval_s: 10.0
    look_back_period_s: 30.0
    smoothing_factor: 1.0
    downscale_delay_s: 300.0
    upscale_delay_s: 30.0
  ray_actor_options:
    resources:
      worker_node: 0.5
  user_config:
    hf_home: /nvme/cache
    model_config:
      max_batch_size: 2
      dtype: float16
      from_pretrained_kwargs: {}
      generation_kwargs:
        do_sample: true
        max_new_tokens: 512
        top_p: 0.92
        top_k: 0
      mirror_bucket_uri: s3://large-dl-models-mirror/models--diegi97--dolly-v2-12b-sharded-bf16/main/
      mode:
        type: DeepSpeed
        use_kernel: false
      name: diegi97/dolly-v2-12b-sharded-bf16
      pipeline_cls: dollyv2
      prompt_format: null
      stopping_tokens: null
    scaling_config:
      num_workers: 2
      num_gpus_per_worker: 1
      num_cpus_per_worker: 4

- name: RouterDeployment
  route_prefix: /
