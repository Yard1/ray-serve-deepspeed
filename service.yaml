# This file was generated using the `serve build` command on Ray v3.0.0.dev0.
name: aviary_v0
ray_serve_config:
  import_path: rayviary.app:entrypoint

  deployments:

  - name: LLMDeployment
    max_concurrent_queries: 4
    autoscaling_config:
      min_replicas: 1
      initial_replicas: 1
      max_replicas: 8
      target_num_ongoing_requests_per_replica: 1.0
      metrics_interval_s: 10.0
      look_back_period_s: 30.0
      smoothing_factor: 1.0
      downscale_delay_s: 300.0
      upscale_delay_s: 30.0
    ray_actor_options:
      resources:
        worker_node: 0.5
    user_config:
      hf_home: /nvme/cache
      model_config:
        max_batch_size: 4
        dtype: float16
        from_pretrained_kwargs: {}
        generation_kwargs:
          do_sample: true
          max_new_tokens: 256
          min_new_tokens: 16
          temperature: 1.0
          top_p: 1.0
          return_token_type_ids: false
        mirror_bucket_uri: s3://large-dl-models-mirror/restricted/models--CarperAI--stable-vicuna-13b-delta/main/
        mode:
          type: DeepSpeed
          use_kernel: false
        name: CarperAI/stable-vicuna-13b-delta
        pipeline_cls: default
        prompt_format: "### Assistant: I am StableVicuna, a large language model created by CarperAI. I am here to chat!\n### Human: {instruction}\n### Assistant: "
        stopping_sequences: [835, [13, 2277], 2]
      scaling_config:
        num_workers: 2
        num_gpus_per_worker: 1
        num_cpus_per_worker: 4

  - name: LLMDeployment_1
    max_concurrent_queries: 4
    autoscaling_config:
      min_replicas: 1
      initial_replicas: 1
      max_replicas: 8
      target_num_ongoing_requests_per_replica: 1.0
      metrics_interval_s: 10.0
      look_back_period_s: 30.0
      smoothing_factor: 1.0
      downscale_delay_s: 300.0
      upscale_delay_s: 30.0
    ray_actor_options:
      resources:
        worker_node: 0.5
    user_config:
      hf_home: /nvme/cache
      model_config:
        max_batch_size: 4
        dtype: float16
        from_pretrained_kwargs: {}
        generation_kwargs:
          do_sample: true
          max_new_tokens: 256
          min_new_tokens: 16
          temperature: 0.7
          return_token_type_ids: false
        mirror_bucket_uri: s3://large-dl-models-mirror/restricted/models--lmsys--vicuna-13b-delta-v1.1/main/
        mode:
          type: DeepSpeed
          use_kernel: false
        name: lmsys/vicuna-13b-delta-v1.1
        pipeline_cls: default
        prompt_format: "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. USER: {instruction} ASSISTANT: "
        stopping_sequences: []
      scaling_config:
        num_workers: 2
        num_gpus_per_worker: 1
        num_cpus_per_worker: 4

  - name: LLMDeployment_2
    max_concurrent_queries: 18
    autoscaling_config:
      min_replicas: 1
      initial_replicas: 1
      max_replicas: 8
      target_num_ongoing_requests_per_replica: 1.0
      metrics_interval_s: 10.0
      look_back_period_s: 30.0
      smoothing_factor: 1.0
      downscale_delay_s: 300.0
      upscale_delay_s: 30.0
    ray_actor_options:
      resources:
        worker_node: 0.5
    user_config:
      hf_home: /nvme/cache
      model_config:
        max_batch_size: 18
        dtype: float16
        from_pretrained_kwargs: {}
        generation_kwargs:
          do_sample: true
          max_new_tokens: 256
          min_new_tokens: 16
          do_sample: true
          top_p: 0.95
          top_k: 1000
          temperature: 1.0
          num_beams: 1
        mirror_bucket_uri: s3://large-dl-models-mirror/models--stabilityai--stablelm-tuned-alpha-7b/main/
        mode:
          type: DeepSpeed
          use_kernel: true
        name: stabilityai/stablelm-tuned-alpha-7b
        pipeline_cls: default
        prompt_format: "<|SYSTEM|># StableLM Tuned (Alpha version)\n- StableLM is a helpful and harmless open-source AI language model developed by StabilityAI.\n- StableLM is excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- StableLM is more than just an information source, StableLM is also able to write poetry, short stories, and make jokes.\n- StableLM will refuse to participate in anything that could harm a human.<|USER|>{instruction}<|ASSISTANT|>"
        stopping_sequences: [50278, 50279, 50277, 1, 0]
      scaling_config:
        num_workers: 2
        num_gpus_per_worker: 1
        num_cpus_per_worker: 4

  - name: LLMDeployment_3
    max_concurrent_queries: 4
    autoscaling_config:
      min_replicas: 1
      initial_replicas: 1
      max_replicas: 8
      target_num_ongoing_requests_per_replica: 1.0
      metrics_interval_s: 10.0
      look_back_period_s: 30.0
      smoothing_factor: 1.0
      downscale_delay_s: 300.0
      upscale_delay_s: 30.0
    ray_actor_options:
      resources:
        worker_node: 0.5
    user_config:
      hf_home: /nvme/cache
      model_config:
        max_batch_size: 4
        dtype: float16
        from_pretrained_kwargs: {}
        generation_kwargs:
          do_sample: true
          max_new_tokens: 256
          min_new_tokens: 16
          top_p: 0.92
          top_k: 0
          temperature: 1.0
        mirror_bucket_uri: s3://large-dl-models-mirror/models--diegi97--dolly-v2-12b-sharded-bf16/main/
        mode:
          type: DeepSpeed
          use_kernel: false
        name: diegi97/dolly-v2-12b-sharded-bf16
        pipeline_cls: default
        prompt_format: "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n### Instruction:\n{instruction}\n### Response:\n"
        stopping_sequences: ["### Response:", "### End"]
      scaling_config:
        num_workers: 2
        num_gpus_per_worker: 1
        num_cpus_per_worker: 4

  - name: LLMDeployment_4
    max_concurrent_queries: 2
    autoscaling_config:
      min_replicas: 1
      initial_replicas: 1
      max_replicas: 8
      target_num_ongoing_requests_per_replica: 1.0
      metrics_interval_s: 10.0
      look_back_period_s: 30.0
      smoothing_factor: 1.0
      downscale_delay_s: 300.0
      upscale_delay_s: 30.0
    ray_actor_options:
      resources:
        worker_node: 0.25
    user_config:
      hf_home: /nvme/cache
      model_config:
        max_batch_size: 2
        dtype: bfloat16
        from_pretrained_kwargs:
          trust_remote_code: true
        generation_kwargs:
          do_sample: true
          max_new_tokens: 256
          min_new_tokens: 16
          top_p: 1.0
          top_k: 0
          temperature: 0.1
          repetition_penalty: 1.1
        mirror_bucket_uri: null
        mode:
          type: SingleDevice
        name: mosaicml/mpt-7b-chat
        pipeline_cls: default
        prompt_format: "<|im_start|>system\n- You are a helpful assistant chatbot trained by MosaicML.\n- You answer questions.\n- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.<|im_end|>\n<|im_start|>user\n{instruction}<|im_end|><|im_start|>assistant\n"
        stopping_sequences: ["<|im_end|>", "<|endoftext|>"]
      scaling_config:
        num_workers: 1
        num_gpus_per_worker: 1
        num_cpus_per_worker: 4

  - name: RouterDeployment
    route_prefix: /
